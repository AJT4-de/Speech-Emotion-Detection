# Speech Emotion Detection Project

A comprehensive machine learning project implementing multiple approaches for Speech Emotion Recognition (SER) using different deep learning and machine learning techniques.

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://python.org)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.15.0-orange)](https://tensorflow.org)
[![Scikit-learn](https://img.shields.io/badge/Scikit--learn-1.4.2-green)](https://scikit-learn.org)
[![License](https://img.shields.io/badge/License-Educational-purple)](#)

## ğŸ¯ Project Overview

This project explores three different state-of-the-art approaches to speech emotion recognition, each leveraging different aspects of audio signal processing and machine learning:

- **ğŸ–¼ï¸ CNN with Mel Spectrograms**: Convolutional Neural Network treating audio as visual spectrograms
- **ğŸŒ³ Random Forest**: Traditional machine learning with comprehensive hand-crafted audio features
- **ğŸ”„ RNN-LSTM**: Recurrent Neural Network capturing temporal dependencies in audio sequences

### ğŸ“ Academic Context
This project is part of a **Masters in Applied Artificial Intelligence** program, demonstrating practical implementation of various AI/ML techniques for real-world audio analysis challenges.

## ğŸ“Š Datasets

The project utilizes two high-quality emotional speech datasets:

### Primary Datasets
- **ğŸ­ RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)**
  - Professional actors expressing 8 different emotions
  - High-quality recordings with consistent acoustic conditions
  - 1,440 audio files from 24 professional actors
  - Balanced gender representation (12 male, 12 female)

- **ğŸŒ Emotion Speech Dataset (ESD)**
  - Multi-language emotional speech samples
  - Includes Chinese language data for cross-linguistic analysis
  - Additional emotional expressions for robust training

### Supported Emotions
| Emotion | RAVDESS Code | ESD Code | Description |
|---------|--------------|----------|-------------|
| ğŸ˜ Neutral | 01 | n | Baseline emotional state |
| ğŸ˜Œ Calm | 02 | - | Relaxed, peaceful |
| ğŸ˜Š Happy | 03 | h | Joyful, pleased |
| ğŸ˜¢ Sad | 04 | s | Sorrowful, melancholy |
| ğŸ˜  Angry | 05 | a | Frustrated, irritated |
| ğŸ˜¨ Fearful | 06 | - | Afraid, anxious |
| ğŸ¤¢ Disgust | 07 | - | Repulsed, disgusted (RAVDESS only) |
| ğŸ˜² Surprise | 08 | u | Astonished, amazed |

**Total Emotions Supported**: 7-8 emotions (depending on dataset)

## ğŸ—ï¸ Project Structure

```
Speech Emotion Detection/
â”œâ”€â”€ ğŸ“„ README.md                  # This comprehensive guide
â”œâ”€â”€ ğŸš« .gitignore                 # Git ignore patterns for datasets/models
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ–¼ï¸ Speech Emotion Checker_CNN-Mel Spectogram/
â”‚   â”œâ”€â”€ config.py                 # Configuration parameters
â”‚   â”œâ”€â”€ data_preprocessing.py     # Audio preprocessing pipeline
â”‚   â”œâ”€â”€ model.py                  # CNN model architecture
â”‚   â”œâ”€â”€ train.py                  # Training script
â”‚   â”œâ”€â”€ evaluate.py               # Model evaluation
â”‚   â”œâ”€â”€ requirements.txt          # Dependencies
â”‚   â”œâ”€â”€ ğŸ“Š datasets/              # Dataset storage (ignored by git)
â”‚   â”‚   â”œâ”€â”€ RAVDESS/
â”‚   â”‚   â””â”€â”€ Emotion Speech Dataset/
â”‚   â””â”€â”€ ğŸ¤– models/                # Trained models and results
â”‚       â”œâ”€â”€ ser_cnn_best_model.keras
â”‚       â”œâ”€â”€ label_encoder.npy
â”‚       â”œâ”€â”€ confusion_matrix.png
â”‚       â””â”€â”€ training_history.png
â”‚
â”œâ”€â”€ ğŸŒ³ Speech Emotion Checker_Random Forest/
â”‚   â”œâ”€â”€ ğŸŒ Speech Emotion Checker-Random Forest(All including chinese)/
â”‚   â”‚   â”œâ”€â”€ config.py             # Configuration
â”‚   â”‚   â”œâ”€â”€ data_loader.py        # Data loading utilities
â”‚   â”‚   â”œâ”€â”€ feature_extractor.py  # Audio feature extraction (196 features)
â”‚   â”‚   â”œâ”€â”€ model_trainer.py      # Random Forest training
â”‚   â”‚   â”œâ”€â”€ predict.py            # Interactive prediction interface
â”‚   â”‚   â”œâ”€â”€ validate_model.py     # Comprehensive model validation
â”‚   â”‚   â”œâ”€â”€ test_prediction.py    # Simple testing script
â”‚   â”‚   â”œâ”€â”€ requirements.txt      # Dependencies
â”‚   â”‚   â”œâ”€â”€ README_Results.md     # Detailed performance analysis
â”‚   â”‚   â”œâ”€â”€ ğŸ“Š RAVDESS_data/      # RAVDESS dataset (ignored by git)
â”‚   â”‚   â”œâ”€â”€ ğŸ“Š Emotion Speech Dataset/ # ESD dataset (ignored by git)
â”‚   â”‚   â””â”€â”€ ğŸ¤– models/            # Trained models
â”‚   â”‚       â”œâ”€â”€ ser_model.pkl     # Random Forest model
â”‚   â”‚       â””â”€â”€ ser_scaler.pkl    # Feature scaler
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ§ª Speech Emotion Checker-Random Forest(Experimental)/
â”‚   â”‚   â”œâ”€â”€ advanced_*.py         # Advanced implementations
â”‚   â”‚   â”œâ”€â”€ optimized_*.py        # Performance optimizations
â”‚   â”‚   â”œâ”€â”€ analyze_dataset.py    # Dataset analysis tools
â”‚   â”‚   â””â”€â”€ [Additional experimental features]
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ¯ Speech Emotion Checker-Random Forest(Only RAVDESS)/
â”‚       â””â”€â”€ [RAVDESS-only implementation for comparison]
â”‚
â””â”€â”€ ğŸ”„ Speech Emotion Checker_RNN-LSTM/
    â”œâ”€â”€ ser_model.py              # LSTM model implementation
    â”œâ”€â”€ predict.py                # Prediction script
    â”œâ”€â”€ create_preprocessing_objects.py  # Preprocessing setup
    â”œâ”€â”€ requirements.txt          # Dependencies
    â”œâ”€â”€ features_cache.pkl        # Cached features (ignored by git)
    â”œâ”€â”€ ğŸ“Š ravdess/               # Dataset (ignored by git)
    â””â”€â”€ ğŸ¤– models/                # Trained models (ignored by git)
```

### ğŸ“ Key Directories
- **ğŸ“Š Dataset folders**: Automatically ignored by Git (see `.gitignore`)
- **ğŸ¤– Model folders**: Contain trained models and artifacts
- **ğŸ“„ Config files**: Centralized configuration for each approach
- **ğŸ§ª Experimental**: Advanced features and optimization attempts

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.8+** 
- **Git** (for cloning and version control)
- **Audio drivers** (for audio processing)
- **CUDA** (optional, for GPU acceleration with deep learning models)

### ğŸ”§ Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/AJT4-de/Speech-Emotion-Detection.git
   cd Speech-Emotion-Detection
   ```

2. **Choose your preferred approach** and navigate to the corresponding directory:

   **ğŸ–¼ï¸ For CNN with Mel Spectrograms:**
   ```bash
   cd "Speech Emotion Checker_CNN-Mel Spectogram"
   pip install -r requirements.txt
   ```

   **ğŸŒ³ For Random Forest (Recommended for beginners):**
   ```bash
   cd "Speech Emotion Checker_Random Forest/Speech Emotion Checker-Random Forest(All including chinese)"
   pip install -r requirements.txt
   ```

   **ğŸ”„ For RNN-LSTM:**
   ```bash
   cd "Speech Emotion Checker_RNN-LSTM"
   pip install -r requirements.txt
   ```

3. **ğŸ“Š Prepare datasets**: 
   - Download RAVDESS and ESD datasets
   - Place audio files in the appropriate dataset directories
   - Follow the expected folder structure (see Dataset Structure section)

4. **ğŸ¯ Train the model**:
   ```bash
   # CNN approach
   python train.py
   
   # Random Forest approach  
   python main.py
   
   # RNN-LSTM approach
   python ser_model.py
   ```

5. **ğŸ”® Make predictions**:
   ```bash
   # Interactive prediction interface
   python predict.py
   
   # Batch evaluation
   python evaluate.py  # (CNN)
   python validate_model.py  # (Random Forest)
   ```

4. **Train the model**:
   - CNN: `python train.py`
   - Random Forest: `python main.py`
   - RNN-LSTM: `python ser_model.py`

5. **Make predictions**:
   - CNN: `python evaluate.py`
   - Random Forest: `python predict.py`
   - RNN-LSTM: `python predict.py`

## ğŸ”¬ Approaches Comparison

| Feature | ğŸ–¼ï¸ CNN Mel-Spectrogram | ğŸŒ³ Random Forest | ğŸ”„ RNN-LSTM |
|---------|-------------------------|------------------|--------------|
| **Technology** | TensorFlow/Keras | Scikit-learn | TensorFlow/Keras |
| **Input Type** | 2D Mel-spectrogram images | 196 hand-crafted features | Sequential audio features |
| **Architecture** | Convolutional layers + pooling | Ensemble of decision trees | LSTM layers + dense |
| **Training Time** | Medium-Long (GPU recommended) | Fast | Medium-Long |
| **Interpretability** | Low | High | Low |
| **Memory Usage** | High | Low | Medium |
| **Real-time Capability** | Medium | High | Medium |

### 1. ğŸ–¼ï¸ CNN with Mel Spectrograms
- **Strengths**: Excellent for capturing spectral patterns, leverages computer vision techniques
- **Input Processing**: Converts audio to 128-band mel-spectrograms (128x130 images)
- **Best For**: Complex pattern recognition, when you have sufficient training data
- **Innovation**: Treats audio analysis as an image classification problem

### 2. ğŸŒ³ Random Forest
- **Strengths**: Fast training, interpretable results, robust to overfitting
- **Feature Set**: 196 comprehensive audio features (MFCC, spectral, temporal, rhythm)
- **Best For**: Quick prototyping, when interpretability is crucial, limited computational resources
- **Reliability**: Most consistent performer across different datasets

### 3. ğŸ”„ RNN-LSTM
- **Strengths**: Captures temporal dependencies, natural fit for sequential audio data
- **Memory Mechanism**: Long Short-Term Memory for learning long-range dependencies
- **Best For**: When temporal patterns are crucial, speech sequence modeling
- **Advanced**: Handles variable-length sequences naturally

## ğŸ“ˆ Performance Results

### ğŸ† Model Performance Summary

| Model | Overall Accuracy | Training Time | Best Emotion | Challenging Emotion |
|-------|------------------|---------------|--------------|-------------------|
| ğŸŒ³ **Random Forest** | **92%** | âš¡ Fast | Calm (100%) | Neutral |
| ğŸ”„ **RNN-LSTM** | **89%** | ğŸ• Medium | Calm (94%) | Anger |
| ğŸ–¼ï¸ **CNN Mel-Spec** | **87%** | ğŸ• Medium | Happy(92%)| Neutral |

### ğŸŒ³ Random Forest (All Datasets) - *Recommended*
- **âœ… Overall Test Accuracy**: 92%
- **ğŸ“Š Total Samples**: 3,938 audio files
- **ğŸ¯ Feature Count**: 196 hand-crafted audio features
- **ğŸ… Best Performance**: Calm emotions (100% accuracy)
- **âš ï¸ Challenge**: Neutral emotion recognition (often misclassified as fearful)
- **ğŸ’¡ Strengths**: Fast inference, interpretable results, robust performance

### ğŸ–¼ï¸ CNN Mel-Spectrogram
- **âœ… Overall Test Accuracy**: 87%
- **ğŸ–¼ï¸ Input Processing**: 128x130 mel-spectrogram images
- **ğŸ—ï¸ Architecture**: Multi-layer CNN with pooling layers
- **ğŸ’» Requirements**: GPU recommended for training
- **ğŸ’¡ Strengths**: Good pattern recognition, end-to-end learning

### ğŸ”„ RNN-LSTM
- **âœ… Overall Test Accuracy**: 89%
- **â° Sequential Processing**: Time-series audio analysis
- **ğŸ§  Memory**: Long Short-Term Memory for temporal patterns
- **ğŸ”— Architecture**: LSTM + Dense layers
- **ğŸ’¡ Strengths**: Temporal dependency modeling, sequence learning

### ğŸ“Š Detailed Performance Analysis

**Random Forest Validation Results** (RAVDESS Subset):
- **Files Tested**: 15 samples from 3 actors
- **Breakdown by Emotion**:
  - âœ… Calm: 100% accuracy (3/3 correct)
  - âŒ Neutral: 0% accuracy (consistently misclassified as fearful)

**Key Insights**:
- **Best Overall**: Random Forest shows highest accuracy and fastest training
- **Most Reliable**: Random Forest with consistent cross-dataset performance  
- **Most Innovative**: CNN treating audio as visual problem
- **Best for Sequences**: RNN-LSTM for temporal pattern analysis

## ğŸ› ï¸ Key Features

- **Multi-dataset Support**: Works with RAVDESS and ESD datasets
- **Multiple Architectures**: Compare different ML/DL approaches
- **Preprocessing Pipeline**: Comprehensive audio preprocessing
- **Feature Engineering**: Extensive audio feature extraction
- **Model Evaluation**: Detailed performance analysis
- **Prediction Interface**: Easy-to-use prediction scripts

## ğŸ“‹ Dependencies

### Common Requirements
- Python 3.8+
- NumPy
- Pandas
- Scikit-learn
- Librosa (audio processing)
- Matplotlib (visualization)
- SoundFile

### Deep Learning Specific
- TensorFlow 2.15.0
- Keras 2.15.0

### Machine Learning Specific
- Joblib
- Seaborn
- Tqdm

## ğŸ“ Dataset Structure

The project expects datasets to be organized as follows:

```
datasets/
â”œâ”€â”€ RAVDESS/
â”‚   â””â”€â”€ Actor_01/ to Actor_24/
â”‚       â””â”€â”€ 03-01-[emotion]-[intensity]-[statement]-[repetition]-[actor].wav
â”‚           # Example: 03-01-06-01-02-01-12.wav
â”‚           # Format: [Modality]-[Channel]-[Emotion]-[Intensity]-[Statement]-[Rep]-[Actor]
â”‚           # Emotion codes: 01=neutral, 02=calm, 03=happy, 04=sad, 05=angry, 06=fearful, 07=disgust, 08=surprised
â”‚
â””â”€â”€ Emotion Speech Dataset/
    â”œâ”€â”€ [Chinese emotional speech files]
    â””â”€â”€ [Additional language variants]
```

### ğŸµ Audio File Naming Convention (RAVDESS)
- **Modality**: 03 (audio-only)
- **Channel**: 01 (speech)
- **Emotion**: 01-08 (see emotion mapping above)
- **Intensity**: 01 (normal), 02 (strong)
- **Statement**: 01 ("Kids are talking by the door"), 02 ("Dogs are sitting by the door")
- **Repetition**: 01 (1st repetition), 02 (2nd repetition)  
- **Actor**: 01-24 (actor identifier)

### ğŸ“‚ Important Notes
- Datasets are automatically excluded from Git commits (see `.gitignore`)
- Download datasets separately and place in appropriate folders
- Ensure proper folder structure for automated data loading

## ï¿½ Technical Documentation

### ï¿½ğŸ”§ Configuration Files
Each approach includes detailed configuration:
- **CNN**: `config.py` - Audio parameters (sample rate, mel bands, training epochs)
- **Random Forest**: `config.py` - Feature extraction settings, emotion mappings
- **RNN-LSTM**: Embedded parameters in model scripts

### ğŸµ Audio Processing Pipeline
1. **Audio Loading**: Librosa-based audio file loading
2. **Preprocessing**: Normalization, resampling to 22kHz
3. **Feature Extraction**: 
   - CNN: Mel-spectrogram generation (128 bands)
   - Random Forest: 196 traditional features (MFCC, spectral, temporal)
   - RNN-LSTM: Sequential feature vectors
4. **Data Augmentation**: Optional noise addition and temporal shifts

### ğŸ§  Model Architectures

**CNN Architecture:**
```
Input (128x130x1) â†’ Conv2D â†’ MaxPool â†’ Conv2D â†’ MaxPool â†’ 
Flatten â†’ Dense(128) â†’ Dropout â†’ Dense(7) â†’ Softmax
```

**Random Forest Configuration:**
```
n_estimators=100, max_depth=None, min_samples_split=2
Feature scaling: StandardScaler
196 features: 13 MFCC + spectral + temporal + rhythm features
```

**RNN-LSTM Structure:**
```
Input â†’ LSTM(64) â†’ Dropout â†’ LSTM(32) â†’ Dense(64) â†’ 
Dropout â†’ Dense(7) â†’ Softmax
```

## ğŸ” Troubleshooting

### Common Issues & Solutions

**Dataset Loading Problems:**
```bash
# Ensure proper folder structure
ls datasets/RAVDESS/Actor_01/  # Should show .wav files
```

**Memory Issues (CNN/RNN):**
```python
# Reduce batch size in config.py
BATCH_SIZE = 16  # Instead of 32
```

**Audio Processing Errors:**
```bash
# Install additional audio codecs
pip install soundfile librosa[display]
```

**Model Loading Failures:**
```python
# Check model file existence
import os
print(os.path.exists('models/ser_model.pkl'))
```

### Performance Optimization Tips
- **GPU Usage**: Enable CUDA for TensorFlow models
- **Feature Caching**: Use cached features for faster training
- **Parallel Processing**: Utilize multiprocessing for feature extraction

## ğŸ“Š Usage Examples

### ğŸ¯ Training Models

**Random Forest (Recommended for beginners):**
```bash
cd "Speech Emotion Checker_Random Forest/Speech Emotion Checker-Random Forest(All including chinese)"
python main.py
# Output: Trained model saved to models/ser_model.pkl
```

**CNN with Mel-Spectrograms:**
```bash
cd "Speech Emotion Checker_CNN-Mel Spectogram"
python train.py
# Output: Model checkpoints and training visualizations
```

**RNN-LSTM:**
```bash
cd "Speech Emotion Checker_RNN-LSTM"
python ser_model.py
# Output: Sequential model with temporal learning
```

### ğŸ”® Making Predictions

**Interactive Prediction Interface:**
```bash
python predict.py
# Follow prompts to input audio file path
# Get real-time emotion prediction with confidence scores
```

**Batch Validation:**
```bash
# Random Forest comprehensive validation
python validate_model.py

# CNN model evaluation  
python evaluate.py

# Test on specific files
python test_prediction.py
```

### ğŸ§ª Advanced Usage

**Experimental Features (Random Forest):**
```bash
cd "Speech Emotion Checker_Random Forest/Speech Emotion Checker-Random Forest(Experimental)"
python advanced_main.py              # Advanced feature extraction
python analyze_dataset.py            # Dataset analysis and visualization
python optimized_main.py             # Performance-optimized version
```

**Dataset Analysis:**
```bash
python check_emotions.py             # Verify emotion distribution
python debug_features.py             # Feature extraction debugging
```

## ğŸš§ Future Improvements & Research Directions

### ğŸ”¬ Immediate Enhancements
1. **ğŸšï¸ Data Augmentation**: 
   - Audio augmentation (noise injection, pitch shifting, time stretching)
   - Synthetic sample generation for data balancing
   
2. **ğŸ¤– Model Optimization**:
   - Hyperparameter tuning with Grid/Random Search
   - Ensemble methods combining all three approaches
   - Cross-validation for robust evaluation

3. **ğŸ¯ Feature Engineering**:
   - Additional audio features (spectral contrast, harmony, percussive)
   - Deep feature extraction using pre-trained models
   - Multi-scale temporal features

### ğŸŒŸ Advanced Research Directions
4. **ğŸŒ Cross-Lingual Analysis**:
   - Language-independent emotion recognition
   - Transfer learning across languages
   - Cultural emotion expression analysis

5. **âš¡ Real-Time Processing**:
   - Streaming audio emotion detection
   - Mobile deployment optimization
   - Edge computing implementation

6. **ğŸ­ Multi-Modal Fusion**:
   - Combine audio with facial expression analysis
   - Text sentiment + speech emotion correlation
   - Physiological signal integration

### ğŸ­ Production Deployment
7. **ğŸŒ Web Interface Development**:
   - Flask/Django web application
   - RESTful API for emotion detection
   - Real-time dashboard with visualizations

8. **ğŸ“± Mobile Application**:
   - React Native/Flutter implementation
   - On-device inference optimization
   - Privacy-preserving local processing

### ğŸ“š Academic Extensions
9. **ğŸ” Interpretability Research**:
   - LIME/SHAP analysis for deep learning models
   - Feature importance visualization
   - Decision boundary analysis

10. **ğŸ“Š Benchmark Comparisons**:
    - Comparison with state-of-the-art models
    - Cross-dataset evaluation protocols
    - Standardized evaluation metrics

## ğŸ¤ Contributing

This project is part of a **Masters in Applied Artificial Intelligence** program. We welcome contributions from:

### ğŸ‘¥ How to Contribute
- **ğŸ› Bug Reports**: Open issues for any bugs found
- **ğŸ’¡ Feature Requests**: Suggest improvements or new features  
- **ğŸ“– Documentation**: Help improve documentation and examples
- **ğŸ”¬ Research**: Share experimental results and findings
- **ğŸ“Š Datasets**: Contribute additional emotional speech datasets

### ğŸ”„ Development Workflow
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### ğŸ“‹ Contribution Guidelines
- Follow existing code style and structure
- Include comprehensive comments and documentation
- Test your changes across different approaches
- Update README if adding new features

## ï¿½ License

This project is developed for **educational and research purposes** as part of academic coursework.

### Usage Rights
- âœ… **Educational Use**: Free for academic and learning purposes
- âœ… **Research**: Permitted for academic research and publications
- âœ… **Modification**: Feel free to adapt and improve
- âš ï¸ **Commercial Use**: Please contact for commercial licensing

## ğŸ™ Acknowledgments

### ğŸ“š Datasets
- **RAVDESS**: Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)
- **Emotion Speech Dataset**: Contributors to the multi-language emotional speech corpus

### ğŸ› ï¸ Technology Stack
- **TensorFlow/Keras Team**: Deep learning framework
- **Scikit-learn Community**: Machine learning library  
- **Librosa Developers**: Audio analysis toolkit
- **Python Community**: Programming language and ecosystem

### ğŸ“ Academic Context
- **University**: Masters in Applied Artificial Intelligence Program
- **Course**: Individual Project in Speech Emotion Recognition
- **Focus Area**: Multi-approach comparison in AI/ML techniques

### ğŸŒŸ Special Thanks
- Academic supervisors and mentors
- Open-source community for tools and libraries
- Researchers advancing emotion recognition field
- Fellow students providing feedback and collaboration

---

## ğŸ“ Contact & Support

For questions, suggestions, or collaboration opportunities:
- **GitHub Issues**: For technical problems and feature requests
- **Academic Inquiries**: Related to research methodology and findings
- **Collaboration**: Open to academic partnerships and improvements

---

**â­ If this project helps your research or learning, please consider giving it a star!**

---

*This project demonstrates the application of multiple AI/ML approaches to a real-world problem, showcasing the strengths and trade-offs of different techniques in speech emotion recognition.*
